# ğŸ”¤ Frequency Estimation & Discounting for Language Models

![PySpark](https://img.shields.io/badge/PySpark-Analytics-orange)
![NLP](https://img.shields.io/badge/NLP-Language%20Modeling-blue)
![Stats](https://img.shields.io/badge/Statistics-Smoothing-green)

## ğŸ“Œ Overview
This project implements **N-gram probability estimators** using PySpark:

- **Maximum Likelihood Estimation (MLE)**
- **Goodâ€“Turing Smoothing**
- **Absolute Discounting (AD)**

Evaluation focuses on **rare-event probability correction**, divergence metrics, and Zipfian behavior.

---

## ğŸ¯ Objectives
- Compare estimator performance at multiple sample sizes  
- Quantify divergence from reference â€œfull corpusâ€ distribution  
- Visualize rank-frequency and tail behavior  

---

## ğŸ› ï¸ Stack
`PySpark` Â· `NumPy` Â· `pandas` Â· `matplotlib`  

---

## ğŸ”¬ Key Findings

### **Goodâ€“Turing**
- Best for long-tail corrections  
- Stabilizes rare-event probability mass  

### **Absolute Discounting**
- Most stable across mid-frequency tokens  

### **MLE**
- Overfits high-frequency words  
- Fails on sparse vocabularies  

### Convergence
All estimators begin stabilizing by **~1 million tokens**.

---

## ğŸ“Š Deliverables
- Zipf curves  
- Divergence plots (KL, JS, L1)  
- Smoothing comparison tables  

---

## ğŸš€ Next Steps
- Extend to **Kneserâ€“Ney smoothing**  
- Implement a small **backoff language model**  
- Integrate into a streaming text pipeline  

---
# Placeholder - content coming soon
